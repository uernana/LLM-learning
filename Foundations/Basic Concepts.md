## Lesson 1: Linear Algebra for Deep Learning

1. Topics:

+ Scalars, vectors, matrices, tensors

+ Matrix multiplication

+ Dot product (important for attention!)

+ Norms & cosine similarity

+ Eigenvalues/eigenvectors (intuition only)

2. Exercises:

+ Compute dot products manually.

+ Write a function in Python to compute cosine similarity.

+ Visualize a vector projection in 2D.

3. Mini Quiz:

+ What is the shape of matrix multiplication A(m×n) × B(n×k)?

+ Why is dot product important in attention mechanisms?

  ## Lesson 1.2 — Calculus for Neural Networks
1. Topics:

+ Derivatives of common functions

+ Gradients & partial derivatives

+ Chain rule

+ Gradient descent intuition

2. Exercises:

+ Derive the gradient of softmax manually.

+ Write gradient descent for f(x) = x².

3. Mini Quiz:

+ What is the gradient?

+ Why is the chain rule essential in backpropagation?

## Lesson 1.3 — Probability & Statistics Essentials
1. Topics:

+ Random variables

+ Expectation & variance

+ Gaussian distribution

+ Conditional probability

+ Log-likelihood

2. Exercises:

+ Compute probability of combined events.

+ Compute expected value of a distribution.

+ Show why log-likelihood is easier than likelihood.

3. Mini Quiz:

+ What is the difference between PDF and PMF?

+ Why do neural networks use log-likelihood?

  ## Lesson 1.4 — Important ML Functions
1. Topics:

+ Softmax

+ Sigmoid & tanh

+ ReLU, GELU

+ Cross-entropy loss

2. Exercises:

+ Implement softmax in Python (no NumPy allowed!)

+ Implement cross-entropy loss for one-hot and non–one-hot labels.

3. Mini Quiz:

+ Why use softmax instead of argmax?

+ Why is cross-entropy loss used for next-token prediction?
