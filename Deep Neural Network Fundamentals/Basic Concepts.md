## Lesson 3.1 — Anatomy of a Neural Network
1. Topics:

+ Neurons & layers

+ Weights & biases

+ Activation functions

+ Loss functions

## Lesson 3.2 — Forward & Backward Propagation
1. Topics:

+ Manual gradient computation

+ Chain rule applied to networks

+ Parameter updates

2. Exercises:
+ Manually compute gradients for a 2-layer NN

+ Implement backprop in pure Python

# Lesson 3.3 — Optimization
1. Topics:

+ Gradient descent

+ Momentum

+ Adam optimizer

+ Learning rate schedules

2. Exercises:

+ Compare SGD vs Adam on simple regression

+ Try different learning rates and show convergence
  
